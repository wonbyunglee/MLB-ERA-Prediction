{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 6ms/step\n",
      "Iteration 1/5 - RMSE: 0.7729627458787913, MAE: 0.6065100218757751, MAPE: 0.14003818475742286\n",
      "2/2 [==============================] - 2s 7ms/step\n",
      "Iteration 2/5 - RMSE: 0.6195219374060895, MAE: 0.5116436891707163, MAPE: 0.11746352442326514\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.8286160839261828, MAE: 0.6789412719484361, MAPE: 0.154131526844394\n",
      "2/2 [==============================] - 2s 6ms/step\n",
      "Iteration 4/5 - RMSE: 0.6456747321934335, MAE: 0.5140864564502051, MAPE: 0.1214958593309348\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027254217F60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "Iteration 5/5 - RMSE: 0.5824454098072865, MAE: 0.45945962709093857, MAPE: 0.10756059404478018\n",
      "Average RMSE: 0.6898441818423567\n",
      "Average MAE: 0.5541282133072143\n",
      "Average MAPE: 0.12813793788015937\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필요 없는 컬럼 제거\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# 결측치 처리 (예: 0으로 대체)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# 2020, 2021, 2022, 2023년에 모두 존재하는 player_id 추출\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "data_2020 = data[data['year'] == 2020]\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "player_ids_2020 = set(data_2020['player_id'].unique())\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2019 & player_ids_2020 & player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data for common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2019, 2020, 2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# 필요한 컬럼 선택 (year 제외)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 시계열 데이터 형태로 변환\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # 시퀀스 길이 설정\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# 학습 데이터와 전체 데이터를 동일하게 설정\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # GRU 모델 정의\n",
    "    model_GRU = Sequential()\n",
    "    model_GRU.add(GRU(64, input_shape=(seq_length, X_train.shape[2]), return_sequences=True))\n",
    "    model_GRU.add(GRU(64, return_sequences=True))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Flatten())\n",
    "    model_GRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_GRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # 컴파일\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_GRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # 조기 종료 콜백\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model_GRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # 2023년 데이터 필터링\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # 2023년 데이터 스케일링\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "\n",
    "    # 시계열 데이터 형태로 변환 (2022년, 2021년, 2020년 데이터를 사용하여 2023년 예측)\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # 2023년 데이터 예측\n",
    "    y_pred_scaled = model_GRU.predict(X_2023_seq)\n",
    "\n",
    "    # 스케일 복원\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 실제 값과 예측 값 비교를 위해 실제 2023년 p_era 값도 복원\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # MAPE 계산\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# 평균 RMSE, MAE, MAPE 계산\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002725C86F9C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 4s 10ms/step\n",
      "Iteration 1/5 - RMSE: 0.6897461246977341, MAE: 0.5294627377343557, MAPE: 0.11958044044624884\n",
      "2/2 [==============================] - 4s 8ms/step\n",
      "Iteration 2/5 - RMSE: 0.6436558921131084, MAE: 0.5086603452288916, MAPE: 0.11887208583179808\n",
      "2/2 [==============================] - 3s 7ms/step\n",
      "Iteration 3/5 - RMSE: 0.5550984067980801, MAE: 0.4145714361705477, MAPE: 0.09696162032805578\n",
      "2/2 [==============================] - 3s 7ms/step\n",
      "Iteration 4/5 - RMSE: 0.7428803676615368, MAE: 0.5711511401524619, MAPE: 0.12778131057497585\n",
      "2/2 [==============================] - 3s 6ms/step\n",
      "Iteration 5/5 - RMSE: 0.5485738571584889, MAE: 0.44003806492638964, MAPE: 0.10450735483195779\n",
      "Average RMSE: 0.6359909296857896\n",
      "Average MAE: 0.49277674484252926\n",
      "Average MAPE: 0.11354056240260728\n"
     ]
    }
   ],
   "source": [
    "# Bi-GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필요 없는 컬럼 제거\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# 결측치 처리 (예: 0으로 대체)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# 2020, 2021, 2022, 2023년에 모두 존재하는 player_id 추출\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "data_2020 = data[data['year'] == 2020]\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "player_ids_2020 = set(data_2020['player_id'].unique())\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2019 & player_ids_2020 & player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data for common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2019, 2020, 2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# 필요한 컬럼 선택 (year 제외)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 시계열 데이터 형태로 변환\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # 시퀀스 길이 설정\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# 학습 데이터와 전체 데이터를 동일하게 설정\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Bidirectional GRU 모델 정의\n",
    "    model_BiGRU = Sequential()\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Flatten())\n",
    "    model_BiGRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # 컴파일\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # 조기 종료 콜백\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model_BiGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # 2023년 데이터 필터링\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # 2023년 데이터 스케일링\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # 시계열 데이터 형태로 변환 (2022년, 2021년, 2020년 데이터를 사용하여 2023년 예측)\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # 2023년 데이터 예측\n",
    "    y_pred_scaled = model_BiGRU.predict(X_2023_seq)\n",
    "\n",
    "    # 스케일 복원\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 실제 값과 예측 값 비교를 위해 실제 2023년 p_era 값도 복원\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # MAPE 계산\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# 평균 RMSE, MAE, MAPE 계산\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 8ms/step\n",
      "Iteration 1/5 - RMSE: 0.552524451200697, MAE: 0.42861291900513665, MAPE: 0.10043675997563091\n",
      "2/2 [==============================] - 6s 6ms/step\n",
      "Iteration 2/5 - RMSE: 0.6966222783702316, MAE: 0.5141495934743731, MAPE: 0.1175620742839745\n",
      "2/2 [==============================] - 5s 6ms/step\n",
      "Iteration 3/5 - RMSE: 0.5851240924638518, MAE: 0.461901823830983, MAPE: 0.10921679903964862\n",
      "2/2 [==============================] - 3s 7ms/step\n",
      "Iteration 4/5 - RMSE: 0.5949093061524124, MAE: 0.43828646160307383, MAPE: 0.10096409016775838\n",
      "2/2 [==============================] - 3s 6ms/step\n",
      "Iteration 5/5 - RMSE: 0.6068429135062862, MAE: 0.4551974016522604, MAPE: 0.10552843592907264\n",
      "Average RMSE: 0.6072046083386958\n",
      "Average MAE: 0.45962963991316536\n",
      "Average MAPE: 0.10674163187921701\n"
     ]
    }
   ],
   "source": [
    "# biattention GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional, Input, Layer, dot, concatenate, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필요 없는 컬럼 제거\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# 결측치 처리 (예: 0으로 대체)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# 2020, 2021, 2022, 2023년에 모두 존재하는 player_id 추출\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "data_2020 = data[data['year'] == 2020]\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "player_ids_2020 = set(data_2020['player_id'].unique())\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2019 & player_ids_2020 & player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data for common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2019, 2020, 2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# 필요한 컬럼 선택 (year 제외)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 시계열 데이터 형태로 변환\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # 시퀀스 길이 설정\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# 학습 데이터와 전체 데이터를 동일하게 설정\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# 커스텀 Attention Layer 정의\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # BiAttention GRU 모델 정의\n",
    "    input_seq = Input(shape=(seq_length, X_train.shape[2]))\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(input_seq)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    # Attention Layer 적용\n",
    "    attn_out = Attention()(x)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation=\"relu\")(attn_out)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model_BiAttGRU = Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "    # 컴파일\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiAttGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # 조기 종료 콜백\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model_BiAttGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # 2023년 데이터 필터링\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # 2023년 데이터 스케일링\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # 시계열 데이터 형태로 변환 (2022년, 2021년, 2020년 데이터를 사용하여 2023년 예측)\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # 2023년 데이터 예측\n",
    "    y_pred_scaled = model_BiAttGRU.predict(X_2023_seq)\n",
    "\n",
    "    # 스케일 복원\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 실제 값과 예측 값 비교를 위해 실제 2023년 p_era 값도 복원\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # MAPE 계산\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# 평균 RMSE, MAE, MAPE 계산\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
