{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 9ms/step\n",
      "Iteration 1/5 - RMSE: 0.7602111356835907, MAE: 0.5888721011933827, MAPE: 0.1344204892184779\n",
      "2/2 [==============================] - 9s 8ms/step\n",
      "Iteration 2/5 - RMSE: 0.7544729999717054, MAE: 0.5595026206970216, MAPE: 0.1303836615839045\n",
      "2/2 [==============================] - 7s 11ms/step\n",
      "Iteration 3/5 - RMSE: 0.7091950448303856, MAE: 0.5325230619642471, MAPE: 0.12194741404880793\n",
      "2/2 [==============================] - 8s 8ms/step\n",
      "Iteration 4/5 - RMSE: 0.8047310179209929, MAE: 0.6148829753815184, MAPE: 0.14013149473644454\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA96759300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 6s 8ms/step\n",
      "Iteration 5/5 - RMSE: 0.7314544589533079, MAE: 0.5709712438734752, MAPE: 0.1354512844549518\n",
      "Average RMSE: 0.7520129314719964\n",
      "Average MAE: 0.573350400621929\n",
      "Average MAPE: 0.13246686880851732\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2020, 2021, 2022, and 2023\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "data_2020 = data[data['year'] == 2020]\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "player_ids_2020 = set(data_2020['player_id'].unique())\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2019 & player_ids_2020 & player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data for common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2019, 2020, 2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the BiLSTM model\n",
    "    model_BiLSTM = Sequential()\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_BiLSTM.add(Dropout(rate=0.5))\n",
    "    model_BiLSTM.add(Flatten())\n",
    "    model_BiLSTM.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiLSTM.add(Dropout(rate=0.5))\n",
    "    model_BiLSTM.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiLSTM.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiLSTM.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_BiLSTM = model_BiLSTM.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled_BiLSTM = model_BiLSTM.predict(X_2023_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_BiLSTM = scaler_y.inverse_transform(y_pred_scaled_BiLSTM)\n",
    "\n",
    "    # Actual 2023 p_era values\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_BiLSTM = np.sqrt(mean_squared_error(y_test_actual, y_pred_BiLSTM))\n",
    "    rmse_list.append(rmse_BiLSTM)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_BiLSTM = mean_absolute_error(y_test_actual, y_pred_BiLSTM)\n",
    "    mae_list.append(mae_BiLSTM)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_BiLSTM = mean_absolute_percentage_error(y_test_actual, y_pred_BiLSTM)\n",
    "    mape_list.append(mape_BiLSTM)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_BiLSTM}, MAE: {mae_BiLSTM}, MAPE: {mape_BiLSTM}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FA8DE92C00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 6s 7ms/step\n",
      "Iteration 1/5 - RMSE: 0.7789135947801072, MAE: 0.5905868117014568, MAPE: 0.13350631336991295\n",
      "2/2 [==============================] - 7s 6ms/step\n",
      "Iteration 2/5 - RMSE: 0.7970165281684056, MAE: 0.6535288120451429, MAPE: 0.1473629462154747\n",
      "2/2 [==============================] - 7s 6ms/step\n",
      "Iteration 3/5 - RMSE: 0.7983161464993712, MAE: 0.6370084329635379, MAPE: 0.13927363604757645\n",
      "2/2 [==============================] - 5s 5ms/step\n",
      "Iteration 4/5 - RMSE: 0.9161846392673888, MAE: 0.7196836102198043, MAPE: 0.15370320785190353\n",
      "2/2 [==============================] - 8s 6ms/step\n",
      "Iteration 5/5 - RMSE: 0.9797251490705379, MAE: 0.7915222803751629, MAPE: 0.17316885401511659\n",
      "Average RMSE: 0.854031211557162\n",
      "Average MAE: 0.678465989461021\n",
      "Average MAPE: 0.14940299149999686\n"
     ]
    }
   ],
   "source": [
    "# CNN-BiLSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2020, 2021, 2022, and 2023\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "data_2020 = data[data['year'] == 2020]\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "player_ids_2020 = set(data_2020['player_id'].unique())\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2019 & player_ids_2020 & player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data for common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2019, 2020, 2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the CNN-BiLSTM model\n",
    "    model_CNN_BiLSTM = Sequential()\n",
    "    model_CNN_BiLSTM.add(Conv1D(filters=64, kernel_size=4, activation='relu', input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_CNN_BiLSTM.add(Flatten())\n",
    "    model_CNN_BiLSTM.add(Dense(64, activation='relu'))\n",
    "    model_CNN_BiLSTM.add(Reshape((1, 64)))\n",
    "    model_CNN_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_CNN_BiLSTM.add(Bidirectional(LSTM(64)))\n",
    "    model_CNN_BiLSTM.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_CNN_BiLSTM.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_CNN_BiLSTM = model_CNN_BiLSTM.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled_CNN_BiLSTM = model_CNN_BiLSTM.predict(X_2023_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_CNN_BiLSTM = scaler_y.inverse_transform(y_pred_scaled_CNN_BiLSTM)\n",
    "\n",
    "    # Actual 2023 p_era values\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_CNN_BiLSTM = np.sqrt(mean_squared_error(y_test_actual, y_pred_CNN_BiLSTM))\n",
    "    rmse_list.append(rmse_CNN_BiLSTM)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_CNN_BiLSTM = mean_absolute_error(y_test_actual, y_pred_CNN_BiLSTM)\n",
    "    mae_list.append(mae_CNN_BiLSTM)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_CNN_BiLSTM = mean_absolute_percentage_error(y_test_actual, y_pred_CNN_BiLSTM)\n",
    "    mape_list.append(mape_CNN_BiLSTM)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_CNN_BiLSTM}, MAE: {mae_CNN_BiLSTM}, MAPE: {mape_CNN_BiLSTM}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 6ms/step\n",
      "Iteration 1/5 - RMSE: 0.6624864263920074, MAE: 0.47990863739498074, MAPE: 0.10781432071169858\n",
      "2/2 [==============================] - 9s 8ms/step\n",
      "Iteration 2/5 - RMSE: 0.6855662935326603, MAE: 0.5174205101860894, MAPE: 0.1165392460293263\n",
      "2/2 [==============================] - 11s 6ms/step\n",
      "Iteration 3/5 - RMSE: 0.6384132019011405, MAE: 0.5014186812204028, MAPE: 0.11750912160510474\n",
      "2/2 [==============================] - 6s 7ms/step\n",
      "Iteration 4/5 - RMSE: 0.6317757541842862, MAE: 0.48232028098333446, MAPE: 0.10713615251770399\n",
      "2/2 [==============================] - 8s 8ms/step\n",
      "Iteration 5/5 - RMSE: 0.6853223662089178, MAE: 0.5158571520305816, MAPE: 0.11814250320357597\n",
      "Average RMSE: 0.6607128084438024\n",
      "Average MAE: 0.4993850523630778\n",
      "Average MAPE: 0.11342826881348192\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM-ED\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "# Extract player_ids that exist in 2020, 2021, 2022, and 2023\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "data_2020 = data[data['year'] == 2020]\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "player_ids_2020 = set(data_2020['player_id'].unique())\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2019 & player_ids_2020 & player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data for common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2019, 2020, 2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the BiLSTM-ED model\n",
    "    model_BiLSTM_ED = Sequential()\n",
    "    model_BiLSTM_ED.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiLSTM_ED.add(RepeatVector(seq_length))\n",
    "    model_BiLSTM_ED.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_BiLSTM_ED.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiLSTM_ED.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_BiLSTM_ED = model_BiLSTM_ED.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled_BiLSTM_ED = model_BiLSTM_ED.predict(X_2023_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_BiLSTM_ED = scaler_y.inverse_transform(y_pred_scaled_BiLSTM_ED[:, -1, :])  # Take the last time step\n",
    "\n",
    "    # Actual 2023 p_era values\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_BiLSTM_ED = np.sqrt(mean_squared_error(y_test_actual, y_pred_BiLSTM_ED))\n",
    "    rmse_list.append(rmse_BiLSTM_ED)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_BiLSTM_ED = mean_absolute_error(y_test_actual, y_pred_BiLSTM_ED)\n",
    "    mae_list.append(mae_BiLSTM_ED)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_BiLSTM_ED = mean_absolute_percentage_error(y_test_actual, y_pred_BiLSTM_ED)\n",
    "    mape_list.append(mape_BiLSTM_ED)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_BiLSTM_ED}, MAE: {mae_BiLSTM_ED}, MAPE: {mape_BiLSTM_ED}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
