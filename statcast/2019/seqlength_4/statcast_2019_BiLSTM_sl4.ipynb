{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 5ms/step\n",
      "Iteration 1/5 - RMSE: 0.9412054359758849, MAE: 0.7064478975985229, MAPE: 0.15660813596255518\n",
      "4/4 [==============================] - 3s 5ms/step\n",
      "Iteration 2/5 - RMSE: 0.8623552279569364, MAE: 0.6531005824113092, MAPE: 0.15260372539601463\n",
      "4/4 [==============================] - 4s 6ms/step\n",
      "Iteration 3/5 - RMSE: 0.6909087817821985, MAE: 0.5413881567947003, MAPE: 0.12544157062429093\n",
      "4/4 [==============================] - 6s 7ms/step\n",
      "Iteration 4/5 - RMSE: 0.8548148874758793, MAE: 0.6681756834623193, MAPE: 0.15462536338983482\n",
      "4/4 [==============================] - 5s 7ms/step\n",
      "Iteration 5/5 - RMSE: 0.806702604249327, MAE: 0.6167536115646363, MAPE: 0.1374021953053235\n",
      "Average RMSE: 0.8311973874880451\n",
      "Average MAE: 0.6371731863662976\n",
      "Average MAPE: 0.1453361981356038\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2015, 2016, 2017, 2018, and 2019\n",
    "data_2015 = data[data['year'] == 2015]\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2015 = set(data_2015['player_id'].unique())\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2015 & player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for the years 2015, 2016, 2017, and 2018\n",
    "final = common_data[common_data['year'].isin([2015, 2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the BiLSTM model\n",
    "    model_BiLSTM = Sequential()\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_BiLSTM.add(Dropout(rate=0.5))\n",
    "    model_BiLSTM.add(Flatten())\n",
    "    model_BiLSTM.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiLSTM.add(Dropout(rate=0.5))\n",
    "    model_BiLSTM.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiLSTM.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiLSTM.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_BiLSTM = model_BiLSTM.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2019 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2019 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2019 data\n",
    "    y_pred_scaled_BiLSTM = model_BiLSTM.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_BiLSTM = scaler_y.inverse_transform(y_pred_scaled_BiLSTM)\n",
    "\n",
    "    # Actual 2019 p_era values\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_BiLSTM = np.sqrt(mean_squared_error(y_test_actual, y_pred_BiLSTM))\n",
    "    rmse_list.append(rmse_BiLSTM)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_BiLSTM = mean_absolute_error(y_test_actual, y_pred_BiLSTM)\n",
    "    mae_list.append(mae_BiLSTM)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_BiLSTM = mean_absolute_percentage_error(y_test_actual, y_pred_BiLSTM)\n",
    "    mape_list.append(mape_BiLSTM)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_BiLSTM}, MAE: {mae_BiLSTM}, MAPE: {mape_BiLSTM}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 5ms/step\n",
      "Iteration 1/5 - RMSE: 0.8065517511945001, MAE: 0.6246055731252461, MAPE: 0.14940905723891118\n",
      "4/4 [==============================] - 5s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.8686547528440962, MAE: 0.6707995301735501, MAPE: 0.15570117602250416\n",
      "4/4 [==============================] - 9s 5ms/step\n",
      "Iteration 3/5 - RMSE: 0.7927439798137556, MAE: 0.5976601560576622, MAPE: 0.13860703409978353\n",
      "4/4 [==============================] - 7s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.8945107051126167, MAE: 0.6923840940699857, MAPE: 0.1622491101339976\n",
      "4/4 [==============================] - 9s 5ms/step\n",
      "Iteration 5/5 - RMSE: 0.8916029911486512, MAE: 0.6772230600709673, MAPE: 0.15390812394711076\n",
      "Average RMSE: 0.850812836022724\n",
      "Average MAE: 0.6525344826994823\n",
      "Average MAPE: 0.15197490028846145\n"
     ]
    }
   ],
   "source": [
    "# CNN-BiLSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data/statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2015, 2016, 2017, 2018, and 2019\n",
    "data_2015 = data[data['year'] == 2015]\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2015 = set(data_2015['player_id'].unique())\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2015 & player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to the years 2015, 2016, 2017, and 2018\n",
    "final = common_data[common_data['year'].isin([2015, 2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the CNN-BiLSTM model\n",
    "    model_CNN_BiLSTM = Sequential()\n",
    "    model_CNN_BiLSTM.add(Conv1D(filters=64, kernel_size=4, activation='relu', input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_CNN_BiLSTM.add(Flatten())\n",
    "    model_CNN_BiLSTM.add(Dense(64, activation='relu'))\n",
    "    model_CNN_BiLSTM.add(Reshape((1, 64)))\n",
    "    model_CNN_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_CNN_BiLSTM.add(Bidirectional(LSTM(64)))\n",
    "    model_CNN_BiLSTM.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_CNN_BiLSTM.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_CNN_BiLSTM = model_CNN_BiLSTM.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2019 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2019 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2019 data\n",
    "    y_pred_scaled_CNN_BiLSTM = model_CNN_BiLSTM.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_CNN_BiLSTM = scaler_y.inverse_transform(y_pred_scaled_CNN_BiLSTM)\n",
    "\n",
    "    # Actual 2019 p_era values\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_CNN_BiLSTM = np.sqrt(mean_squared_error(y_test_actual, y_pred_CNN_BiLSTM))\n",
    "    rmse_list.append(rmse_CNN_BiLSTM)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_CNN_BiLSTM = mean_absolute_error(y_test_actual, y_pred_CNN_BiLSTM)\n",
    "    mae_list.append(mae_CNN_BiLSTM)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_CNN_BiLSTM = mean_absolute_percentage_error(y_test_actual, y_pred_CNN_BiLSTM)\n",
    "    mape_list.append(mape_CNN_BiLSTM)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_CNN_BiLSTM}, MAE: {mae_CNN_BiLSTM}, MAPE: {mape_CNN_BiLSTM}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 10s 6ms/step\n",
      "Iteration 1/5 - RMSE: 0.6844446849027701, MAE: 0.5183175593464314, MAPE: 0.11990969998030102\n",
      "4/4 [==============================] - 24s 9ms/step\n",
      "Iteration 2/5 - RMSE: 0.7104698971360792, MAE: 0.5425713931812959, MAPE: 0.12738539976202248\n",
      "4/4 [==============================] - 10s 6ms/step\n",
      "Iteration 3/5 - RMSE: 0.6845238829238423, MAE: 0.5169340154904277, MAPE: 0.12044582822707123\n",
      "4/4 [==============================] - 11s 6ms/step\n",
      "Iteration 4/5 - RMSE: 0.6931236686723767, MAE: 0.5197071018539557, MAPE: 0.11879152747371174\n",
      "4/4 [==============================] - 12s 6ms/step\n",
      "Iteration 5/5 - RMSE: 0.6909088590368059, MAE: 0.5115442158194149, MAPE: 0.12025878946791248\n",
      "Average RMSE: 0.6926941985343749\n",
      "Average MAE: 0.5218148571383051\n",
      "Average MAPE: 0.12135824898220379\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM-ED\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data/statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "# Extract player_ids that exist in 2015, 2016, 2017, 2018, and 2019\n",
    "data_2015 = data[data['year'] == 2015]\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2015 = set(data_2015['player_id'].unique())\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2015 & player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_ids\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to the years 2015, 2016, 2017, and 2018\n",
    "final = common_data[common_data['year'].isin([2015, 2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 4  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the BiLSTM-ED model\n",
    "    model_BiLSTM_ED = Sequential()\n",
    "    model_BiLSTM_ED.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiLSTM_ED.add(RepeatVector(seq_length))\n",
    "    model_BiLSTM_ED.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_BiLSTM_ED.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiLSTM_ED.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_BiLSTM_ED = model_BiLSTM_ED.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2019 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2019 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2019 data\n",
    "    y_pred_scaled_BiLSTM_ED = model_BiLSTM_ED.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_BiLSTM_ED = scaler_y.inverse_transform(y_pred_scaled_BiLSTM_ED[:, -1, :])  # Take the last time step\n",
    "\n",
    "    # Actual 2019 p_era values\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_BiLSTM_ED = np.sqrt(mean_squared_error(y_test_actual, y_pred_BiLSTM_ED))\n",
    "    rmse_list.append(rmse_BiLSTM_ED)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_BiLSTM_ED = mean_absolute_error(y_test_actual, y_pred_BiLSTM_ED)\n",
    "    mae_list.append(mae_BiLSTM_ED)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_BiLSTM_ED = mean_absolute_percentage_error(y_test_actual, y_pred_BiLSTM_ED)\n",
    "    mape_list.append(mape_BiLSTM_ED)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_BiLSTM_ED}, MAE: {mae_BiLSTM_ED}, MAPE: {mape_BiLSTM_ED}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
