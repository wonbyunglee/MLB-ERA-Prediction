{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.64207742039683, MAE: 0.49286400456513674, MAPE: 0.12666272000977957\n",
      "7/7 [==============================] - 3s 5ms/step\n",
      "Iteration 2/5 - RMSE: 0.6558673385973443, MAE: 0.48709160830293385, MAPE: 0.11928426977630992\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.662041938220457, MAE: 0.4939851087118898, MAPE: 0.12147454432174105\n",
      "7/7 [==============================] - 3s 5ms/step\n",
      "Iteration 4/5 - RMSE: 0.6667286966473946, MAE: 0.5052530213551861, MAPE: 0.12490129183255383\n",
      "7/7 [==============================] - 3s 5ms/step\n",
      "Iteration 5/5 - RMSE: 0.649345065053282, MAE: 0.485129535900695, MAPE: 0.12192262354929702\n",
      "Average RMSE: 0.6552120917830616\n",
      "Average MAE: 0.49286465576716837\n",
      "Average MAPE: 0.12284908989793628\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2020, 2021, 2022, and 2023\n",
    "\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# 공통 player_id에 해당하는 데이터 추출\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# 2020, 2021, 2022년에 해당하는 데이터만 추출\n",
    "final = common_data[common_data['year'].isin([2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the BiLSTM model\n",
    "    model_BiLSTM = Sequential()\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_BiLSTM.add(Dropout(rate=0.5))\n",
    "    model_BiLSTM.add(Flatten())\n",
    "    model_BiLSTM.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiLSTM.add(Dropout(rate=0.5))\n",
    "    model_BiLSTM.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiLSTM.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiLSTM.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_BiLSTM = model_BiLSTM.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled_BiLSTM = model_BiLSTM.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_BiLSTM = scaler_y.inverse_transform(y_pred_scaled_BiLSTM)\n",
    "\n",
    "    # Actual 2023 p_era values\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_BiLSTM = np.sqrt(mean_squared_error(y_test_actual, y_pred_BiLSTM))\n",
    "    rmse_list.append(rmse_BiLSTM)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_BiLSTM = mean_absolute_error(y_test_actual, y_pred_BiLSTM)\n",
    "    mae_list.append(mae_BiLSTM)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_BiLSTM = mean_absolute_percentage_error(y_test_actual, y_pred_BiLSTM)\n",
    "    mape_list.append(mape_BiLSTM)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_BiLSTM}, MAE: {mae_BiLSTM}, MAPE: {mape_BiLSTM}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.710998300193906, MAE: 0.5492062564194203, MAPE: 0.1367936997508013\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.7602523504596429, MAE: 0.5659245693257876, MAPE: 0.13980881592219693\n",
      "7/7 [==============================] - 3s 3ms/step\n",
      "Iteration 3/5 - RMSE: 0.725216573979595, MAE: 0.5545296509351049, MAPE: 0.1416289720833737\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.7180199837853797, MAE: 0.5378305581850665, MAPE: 0.1341590610865572\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.7100727146693395, MAE: 0.5529182872389045, MAPE: 0.14397579935648383\n",
      "Average RMSE: 0.7249119846175727\n",
      "Average MAE: 0.5520818644208567\n",
      "Average MAPE: 0.13927326963988257\n"
     ]
    }
   ],
   "source": [
    "# CNN-BiLSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2020, 2021, 2022, and 2023\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# 공통 player_id에 해당하는 데이터 추출\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# 2020, 2021, 2022년에 해당하는 데이터만 추출\n",
    "final = common_data[common_data['year'].isin([2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the CNN-BiLSTM model\n",
    "    model_CNN_BiLSTM = Sequential()\n",
    "    model_CNN_BiLSTM.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_CNN_BiLSTM.add(Flatten())\n",
    "    model_CNN_BiLSTM.add(Dense(64, activation='relu'))\n",
    "    model_CNN_BiLSTM.add(Reshape((1, 64)))\n",
    "    model_CNN_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_CNN_BiLSTM.add(Bidirectional(LSTM(64)))\n",
    "    model_CNN_BiLSTM.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_CNN_BiLSTM.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_CNN_BiLSTM = model_CNN_BiLSTM.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled_CNN_BiLSTM = model_CNN_BiLSTM.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_CNN_BiLSTM = scaler_y.inverse_transform(y_pred_scaled_CNN_BiLSTM)\n",
    "\n",
    "    # Actual 2023 p_era values\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_CNN_BiLSTM = np.sqrt(mean_squared_error(y_test_actual, y_pred_CNN_BiLSTM))\n",
    "    rmse_list.append(rmse_CNN_BiLSTM)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_CNN_BiLSTM = mean_absolute_error(y_test_actual, y_pred_CNN_BiLSTM)\n",
    "    mae_list.append(mae_CNN_BiLSTM)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_CNN_BiLSTM = mean_absolute_percentage_error(y_test_actual, y_pred_CNN_BiLSTM)\n",
    "    mape_list.append(mape_CNN_BiLSTM)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_CNN_BiLSTM}, MAE: {mae_CNN_BiLSTM}, MAPE: {mape_CNN_BiLSTM}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.6333283919936862, MAE: 0.4694738808061395, MAPE: 0.11756820454601767\n",
      "7/7 [==============================] - 4s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.6504622338443852, MAE: 0.4894571054833276, MAPE: 0.12220749168746405\n",
      "7/7 [==============================] - 5s 6ms/step\n",
      "Iteration 3/5 - RMSE: 0.6486508268305735, MAE: 0.4859738651130881, MAPE: 0.12109112459647253\n",
      "7/7 [==============================] - 5s 5ms/step\n",
      "Iteration 4/5 - RMSE: 0.6438112125164887, MAE: 0.48441665215151647, MAPE: 0.12343656238647159\n",
      "7/7 [==============================] - 6s 7ms/step\n",
      "Iteration 5/5 - RMSE: 0.6409778977613776, MAE: 0.483590057598693, MAPE: 0.12181929508922877\n",
      "Average RMSE: 0.6434461125893022\n",
      "Average MAE: 0.4825823122305529\n",
      "Average MAPE: 0.12122453566113092\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM-ED\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Flatten, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_ids that exist in 2020, 2021, 2022, and 2023\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# 공통 player_id에 해당하는 데이터 추출\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# 2020, 2021, 2022년에 해당하는 데이터만 추출\n",
    "final = common_data[common_data['year'].isin([2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Split independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Function to create sequences for time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize the BiLSTM-ED model\n",
    "    model_BiLSTM_ED = Sequential()\n",
    "    model_BiLSTM_ED.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiLSTM_ED.add(RepeatVector(seq_length))\n",
    "    model_BiLSTM_ED.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model_BiLSTM_ED.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "    # Compile the model\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiLSTM_ED.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_BiLSTM_ED = model_BiLSTM_ED.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Function to create sequences for prediction\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled_BiLSTM_ED = model_BiLSTM_ED.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    y_pred_BiLSTM_ED = scaler_y.inverse_transform(y_pred_scaled_BiLSTM_ED[:, -1, :])  # Take the last time step\n",
    "\n",
    "    # Actual 2023 p_era values\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_BiLSTM_ED = np.sqrt(mean_squared_error(y_test_actual, y_pred_BiLSTM_ED))\n",
    "    rmse_list.append(rmse_BiLSTM_ED)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae_BiLSTM_ED = mean_absolute_error(y_test_actual, y_pred_BiLSTM_ED)\n",
    "    mae_list.append(mae_BiLSTM_ED)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_BiLSTM_ED = mean_absolute_percentage_error(y_test_actual, y_pred_BiLSTM_ED)\n",
    "    mape_list.append(mape_BiLSTM_ED)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse_BiLSTM_ED}, MAE: {mae_BiLSTM_ED}, MAPE: {mape_BiLSTM_ED}')\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
