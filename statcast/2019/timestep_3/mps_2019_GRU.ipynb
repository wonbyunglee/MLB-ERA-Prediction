{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 3ms/step\n",
      "Iteration 1/5 - RMSE: 0.9236361617217328, MAE: 0.7047771661622184, MAPE: 0.15977491934244392\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.7537981419598678, MAE: 0.568528867761294, MAPE: 0.13964434107160964\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.9273566065065478, MAE: 0.7056780060983839, MAPE: 0.1605734692951875\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.9459875274561333, MAE: 0.7020318584498905, MAPE: 0.15903946203194994\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.707653094036329, MAE: 0.5387915950729734, MAPE: 0.1330136471803797\n",
      "Average RMSE: 0.8516863063361221\n",
      "Average MAE: 0.643961498708952\n",
      "Average MAPE: 0.15040916778431412\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# 2020, 2021, 2022, 2023년에 모두 존재하는 player_id 추출\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# 공통 player_id에 해당하는 데이터 추출\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# 2020, 2021, 2022년에 해당하는 데이터만 추출\n",
    "final = common_data[common_data['year'].isin([2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# 필요한 컬럼 선택 (year 제외)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 시계열 데이터 형태로 변환\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 3  # 시퀀스 길이 설정\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# 학습 데이터와 전체 데이터를 동일하게 설정\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # GRU 모델 정의\n",
    "    model_GRU = Sequential()\n",
    "    model_GRU.add(GRU(64, input_shape=(seq_length, X_train.shape[2]), return_sequences=True))\n",
    "    model_GRU.add(GRU(64, return_sequences=True))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Flatten())\n",
    "    model_GRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_GRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # 컴파일\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_GRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # 조기 종료 콜백\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model_GRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # 2023년 데이터 필터링\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # 2023년 데이터 스케일링\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # 시계열 데이터 형태로 변환 (2022년, 2021년, 2020년 데이터를 사용하여 2023년 예측)\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # 2023년 데이터 예측\n",
    "    y_pred_scaled = model_GRU.predict(X_2019_seq)\n",
    "\n",
    "    # 스케일 복원\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 실제 값과 예측 값 비교를 위해 실제 2023년 p_era 값도 복원\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # MAPE 계산\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# 평균 RMSE, MAE, MAPE 계산\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.9580724704335666, MAE: 0.7013389243966058, MAPE: 0.16542100776956004\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 2/5 - RMSE: 1.0935505856716532, MAE: 0.8072620034217834, MAPE: 0.18377815316577267\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.9452087546452052, MAE: 0.7189686396576109, MAPE: 0.16373867999489597\n",
      "6/6 [==============================] - 2s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.9727881165235039, MAE: 0.7189825087785721, MAPE: 0.1631596165864415\n",
      "6/6 [==============================] - 2s 5ms/step\n",
      "Iteration 5/5 - RMSE: 0.9368445781787402, MAE: 0.6948843218599047, MAPE: 0.1609120562526219\n",
      "Average RMSE: 0.9812929010905338\n",
      "Average MAE: 0.7282872796228954\n",
      "Average MAPE: 0.1674019027538584\n"
     ]
    }
   ],
   "source": [
    "# Bi-GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# 2020, 2021, 2022, 2023년에 모두 존재하는 player_id 추출\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# 공통 player_id에 해당하는 데이터 추출\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# 2020, 2021, 2022년에 해당하는 데이터만 추출\n",
    "final = common_data[common_data['year'].isin([2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# 필요한 컬럼 선택 (year 제외)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 시계열 데이터 형태로 변환\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 3  # 시퀀스 길이 설정\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# 학습 데이터와 전체 데이터를 동일하게 설정\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Bidirectional GRU 모델 정의\n",
    "    model_BiGRU = Sequential()\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Flatten())\n",
    "    model_BiGRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # 컴파일\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # 조기 종료 콜백\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model_BiGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # 2023년 데이터 필터링\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # 2023년 데이터 스케일링\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # 시계열 데이터 형태로 변환 (2022년, 2021년, 2020년 데이터를 사용하여 2023년 예측)\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # 2023년 데이터 예측\n",
    "    y_pred_scaled = model_BiGRU.predict(X_2019_seq)\n",
    "\n",
    "    # 스케일 복원\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 실제 값과 예측 값 비교를 위해 실제 2023년 p_era 값도 복원\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # MAPE 계산\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# 평균 RMSE, MAE, MAPE 계산\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 5ms/step\n",
      "Iteration 1/5 - RMSE: 0.6378037206790037, MAE: 0.48193808172430314, MAPE: 0.12379188630937298\n",
      "6/6 [==============================] - 4s 5ms/step\n",
      "Iteration 2/5 - RMSE: 0.6578678830669759, MAE: 0.4998492246866226, MAPE: 0.12661211106636847\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.6732611287228882, MAE: 0.5098395718847002, MAPE: 0.13091325251290842\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.6845110097740783, MAE: 0.5367850350765955, MAPE: 0.14000834586378907\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.6708417788926695, MAE: 0.5320009775956472, MAPE: 0.1356440963991435\n",
      "Average RMSE: 0.6648571042271231\n",
      "Average MAE: 0.5120825781935737\n",
      "Average MAPE: 0.1313939384303165\n"
     ]
    }
   ],
   "source": [
    "# biattention GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional, Input, Layer, dot, concatenate, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "file_path = 'C:\\\\Users\\\\co279\\\\mp_statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# 2020, 2021, 2022, 2023년에 모두 존재하는 player_id 추출\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# 공통 player_id에 해당하는 데이터 추출\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# 2020, 2021, 2022년에 해당하는 데이터만 추출\n",
    "final = common_data[common_data['year'].isin([2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# 필요한 컬럼 선택 (year 제외)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 시계열 데이터 형태로 변환\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 3  # 시퀀스 길이 설정\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# 학습 데이터와 전체 데이터를 동일하게 설정\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# 커스텀 Attention Layer 정의\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # BiAttention GRU 모델 정의\n",
    "    input_seq = Input(shape=(seq_length, X_train.shape[2]))\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(input_seq)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    # Attention Layer 적용\n",
    "    attn_out = Attention()(x)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation=\"relu\")(attn_out)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model_BiAttGRU = Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "    # 컴파일\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiAttGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # 조기 종료 콜백\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model_BiAttGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # 2023년 데이터 필터링\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # 2023년 데이터 스케일링\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # 시계열 데이터 형태로 변환 (2022년, 2021년, 2020년 데이터를 사용하여 2023년 예측)\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # 2023년 데이터 예측\n",
    "    y_pred_scaled = model_BiAttGRU.predict(X_2019_seq)\n",
    "\n",
    "    # 스케일 복원\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 실제 값과 예측 값 비교를 위해 실제 2023년 p_era 값도 복원\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # MAPE 계산\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# 평균 RMSE, MAE, MAPE 계산\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
