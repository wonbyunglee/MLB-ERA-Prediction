{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 3ms/step\n",
      "Iteration 1/5 - RMSE: 0.9236361617217328, MAE: 0.7047771661622184, MAPE: 0.15977491934244392\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.7537981419598678, MAE: 0.568528867761294, MAPE: 0.13964434107160964\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.9273566065065478, MAE: 0.7056780060983839, MAPE: 0.1605734692951875\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.9459875274561333, MAE: 0.7020318584498905, MAPE: 0.15903946203194994\n",
      "6/6 [==============================] - 1s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.707653094036329, MAE: 0.5387915950729734, MAPE: 0.1330136471803797\n",
      "Average RMSE: 0.8516863063361221\n",
      "Average MAE: 0.643961498708952\n",
      "Average MAPE: 0.15040916778431412\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_id present in all years 2016, 2017, 2018, 2019\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to years 2016, 2017, 2018\n",
    "final = common_data[common_data['year'].isin([2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 3  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define GRU model\n",
    "    model_GRU = Sequential()\n",
    "    model_GRU.add(GRU(64, input_shape=(seq_length, X_train.shape[2]), return_sequences=True))\n",
    "    model_GRU.add(GRU(64, return_sequences=True))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Flatten())\n",
    "    model_GRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_GRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_GRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_GRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2019 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2019 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Convert to time series data (use 2016, 2017, 2018 data to predict 2019)\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2019 data\n",
    "    y_pred_scaled = model_GRU.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2019 p_era values for comparison\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.9580724704335666, MAE: 0.7013389243966058, MAPE: 0.16542100776956004\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 2/5 - RMSE: 1.0935505856716532, MAE: 0.8072620034217834, MAPE: 0.18377815316577267\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.9452087546452052, MAE: 0.7189686396576109, MAPE: 0.16373867999489597\n",
      "6/6 [==============================] - 2s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.9727881165235039, MAE: 0.7189825087785721, MAPE: 0.1631596165864415\n",
      "6/6 [==============================] - 2s 5ms/step\n",
      "Iteration 5/5 - RMSE: 0.9368445781787402, MAE: 0.6948843218599047, MAPE: 0.1609120562526219\n",
      "Average RMSE: 0.9812929010905338\n",
      "Average MAE: 0.7282872796228954\n",
      "Average MAPE: 0.1674019027538584\n"
     ]
    }
   ],
   "source": [
    "# Bi-GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_id present in all years 2016, 2017, 2018, 2019\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to years 2016, 2017, 2018\n",
    "final = common_data[common_data['year'].isin([2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 3  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define Bidirectional GRU model\n",
    "    model_BiGRU = Sequential()\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Flatten())\n",
    "    model_BiGRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_BiGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter data for the year 2019\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale data for the year 2019\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # Convert to time series data (using data from 2016, 2017, 2018 to predict 2019)\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict data for the year 2019\n",
    "    y_pred_scaled = model_BiGRU.predict(X_2019_seq)\n",
    "\n",
    "    # Inverse scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2019 p_era values for comparison\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 5ms/step\n",
      "Iteration 1/5 - RMSE: 0.6378037206790037, MAE: 0.48193808172430314, MAPE: 0.12379188630937298\n",
      "6/6 [==============================] - 4s 5ms/step\n",
      "Iteration 2/5 - RMSE: 0.6578678830669759, MAE: 0.4998492246866226, MAPE: 0.12661211106636847\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.6732611287228882, MAE: 0.5098395718847002, MAPE: 0.13091325251290842\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.6845110097740783, MAE: 0.5367850350765955, MAPE: 0.14000834586378907\n",
      "6/6 [==============================] - 3s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.6708417788926695, MAE: 0.5320009775956472, MAPE: 0.1356440963991435\n",
      "Average RMSE: 0.6648571042271231\n",
      "Average MAE: 0.5120825781935737\n",
      "Average MAPE: 0.1313939384303165\n"
     ]
    }
   ],
   "source": [
    "# biattention GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional, Input, Layer, dot, concatenate, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/statcast.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "if 'pitch_hand' in data:\n",
    "    data = pd.get_dummies(data, columns=['pitch_hand'], drop_first=True)\n",
    "\n",
    "# Extract player_id that exists in 2016, 2017, 2018, 2019\n",
    "data_2016 = data[data['year'] == 2016]\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2016 = set(data_2016['player_id'].unique())\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2016 & player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to 2016, 2017, 2018\n",
    "final = common_data[common_data['year'].isin([2016, 2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data format\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 3  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Define custom Attention Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define BiAttention GRU model\n",
    "    input_seq = Input(shape=(seq_length, X_train.shape[2]))\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(input_seq)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    # Apply Attention Layer\n",
    "    attn_out = Attention()(x)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation=\"relu\")(attn_out)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model_BiAttGRU = Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiAttGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_BiAttGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter data for 2019\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale data for 2019\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # Convert to time series data format (using 2016, 2017, 2018 data to predict 2019)\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict data for 2019\n",
    "    y_pred_scaled = model_BiAttGRU.predict(X_2019_seq)\n",
    "\n",
    "    # Restore scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2019 p_era values for comparison\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
