{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 1/5 - RMSE: 0.5776028636273925, MAE: 0.45864543818045345, MAPE: 0.13372412384799126\n",
      "7/7 [==============================] - 1s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.6152881710464299, MAE: 0.4938841767933057, MAPE: 0.1374930016468013\n",
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 3/5 - RMSE: 0.5597376973243573, MAE: 0.45045907458245465, MAPE: 0.1302327847380244\n",
      "7/7 [==============================] - 1s 3ms/step\n",
      "Iteration 4/5 - RMSE: 0.7067945181053076, MAE: 0.5754042684859123, MAPE: 0.1515705773617338\n",
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 5/5 - RMSE: 0.6905860731242487, MAE: 0.5490971256799744, MAPE: 0.15196961554435282\n",
      "Average RMSE: 0.6300018646455472\n",
      "Average MAE: 0.5054980167444201\n",
      "Average MAPE: 0.14099802062778072\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/basic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Handle missing values (e.g., replace with 0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Extract player_id that exists in 2021, 2022, and 2023\n",
    "\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to 2021 and 2022\n",
    "final = common_data[common_data['year'].isin([2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data format\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define GRU model\n",
    "    model_GRU = Sequential()\n",
    "    model_GRU.add(GRU(64, input_shape=(seq_length, X_train.shape[2]), return_sequences=True))\n",
    "    model_GRU.add(GRU(64, return_sequences=True))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Flatten())\n",
    "    model_GRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_GRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_GRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_GRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "\n",
    "    # Convert to time series data format (use 2022, 2021, 2020 data to predict 2023)\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled = model_GRU.predict(X_2023_seq)\n",
    "\n",
    "    # Restore scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2023 p_era values for comparison\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 1/5 - RMSE: 0.6821283779163654, MAE: 0.5302002467740561, MAPE: 0.14744768163955235\n",
      "7/7 [==============================] - 4s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.6267220845736746, MAE: 0.5039213381642881, MAPE: 0.13752878082172879\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.6815939988469304, MAE: 0.5489307756009308, MAPE: 0.14951244730901428\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.5769444432589571, MAE: 0.4687038656594096, MAPE: 0.13371904287577577\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.6295517421492851, MAE: 0.5193267593752359, MAPE: 0.14255871659247274\n",
      "Average RMSE: 0.6393881293490425\n",
      "Average MAE: 0.5142165971147842\n",
      "Average MAPE: 0.14215333384770878\n"
     ]
    }
   ],
   "source": [
    "# Bi-GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/basic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Handle missing values (e.g., replace with 0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Extract player_id that exists in 2020, 2021, 2022, and 2023\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define Bidirectional GRU model\n",
    "    model_BiGRU = Sequential()\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Flatten())\n",
    "    model_BiGRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_BiGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # Convert to time series data (use 2022, 2021, 2020 data to predict 2023)\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled = model_BiGRU.predict(X_2023_seq)\n",
    "\n",
    "    # Inverse scaling\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2023 p_era values for comparison\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.6281701094352593, MAE: 0.491720215525604, MAPE: 0.14715397958528637\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.5692956987393294, MAE: 0.4489616589384954, MAPE: 0.13227411153647847\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 3/5 - RMSE: 0.5805149110707362, MAE: 0.4590998698893376, MAPE: 0.13222518080925422\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.5704166707448887, MAE: 0.4533328552522521, MAPE: 0.13504555135881652\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.5759041050813294, MAE: 0.46007183063433355, MAPE: 0.1328556946580225\n",
      "Average RMSE: 0.5848602990143086\n",
      "Average MAE: 0.4626372860480045\n",
      "Average MAPE: 0.1359109035895716\n"
     ]
    }
   ],
   "source": [
    "# biattention GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional, Input, Layer, dot, concatenate, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/basic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Handle missing values (e.g., replace with 0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Extract player_id that exists in 2020, 2021, 2022, and 2023\n",
    "\n",
    "data_2021 = data[data['year'] == 2021]\n",
    "data_2022 = data[data['year'] == 2022]\n",
    "data_2023 = data[data['year'] == 2023]\n",
    "\n",
    "player_ids_2021 = set(data_2021['player_id'].unique())\n",
    "player_ids_2022 = set(data_2022['player_id'].unique())\n",
    "player_ids_2023 = set(data_2023['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2021 & player_ids_2022 & player_ids_2023\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to 2020, 2021, and 2022\n",
    "final = common_data[common_data['year'].isin([2021, 2022])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data format\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Define custom Attention Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define BiAttention GRU model\n",
    "    input_seq = Input(shape=(seq_length, X_train.shape[2]))\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(input_seq)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    # Apply Attention Layer\n",
    "    attn_out = Attention()(x)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation=\"relu\")(attn_out)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model_BiAttGRU = Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiAttGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_BiAttGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_23 = common_data[common_data['year'] == 2023]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2023_scaled = scaler_X.transform(data_23[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # Convert to time series data format (use 2022, 2021, 2020 data to predict 2023)\n",
    "    X_2023_seq = create_sequences_for_prediction(X_2023_scaled, seq_length)\n",
    "\n",
    "    # Predict 2023 data\n",
    "    y_pred_scaled = model_BiAttGRU.predict(X_2023_seq)\n",
    "\n",
    "    # Restore scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2023 p_era values for comparison\n",
    "    y_test_actual = data_23[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
