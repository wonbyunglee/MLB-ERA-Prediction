{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 1/5 - RMSE: 0.7519825475041285, MAE: 0.5730238836790834, MAPE: 0.1377430506869692\n",
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 2/5 - RMSE: 0.8168738690630473, MAE: 0.626708437949419, MAPE: 0.14852364415866584\n",
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 3/5 - RMSE: 0.7324907843798816, MAE: 0.56259752286332, MAPE: 0.13629775880564518\n",
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 4/5 - RMSE: 0.7041087788862961, MAE: 0.5314330304307597, MAPE: 0.12684143341090956\n",
      "7/7 [==============================] - 2s 3ms/step\n",
      "Iteration 5/5 - RMSE: 0.7288539400495395, MAE: 0.5609182009952409, MAPE: 0.1365205898853061\n",
      "Average RMSE: 0.7468619839765785\n",
      "Average MAE: 0.5709362151835646\n",
      "Average MAPE: 0.1371852953894992\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/basic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Handle missing values (e.g., replace with 0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Extract player_id that exists in 2017, 2018, 2019\n",
    "\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to 2017, 2018\n",
    "final = common_data[common_data['year'].isin([2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define GRU model\n",
    "    model_GRU = Sequential()\n",
    "    model_GRU.add(GRU(64, input_shape=(seq_length, X_train.shape[2]), return_sequences=True))\n",
    "    model_GRU.add(GRU(64, return_sequences=True))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Flatten())\n",
    "    model_GRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_GRU.add(Dropout(rate=0.5))\n",
    "    model_GRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_GRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_GRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_GRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2019 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2019 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "\n",
    "    # Convert to time series data\n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2019 data\n",
    "    y_pred_scaled = model_GRU.predict(X_2019_seq)\n",
    "\n",
    "    # Restore scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2019 p_era values for comparison\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 3ms/step\n",
      "Iteration 1/5 - RMSE: 0.855567199058438, MAE: 0.6646598283095019, MAPE: 0.15481510488527173\n",
      "7/7 [==============================] - 3s 3ms/step\n",
      "Iteration 2/5 - RMSE: 0.6871707793195405, MAE: 0.5109948701092175, MAPE: 0.12537251863773832\n",
      "7/7 [==============================] - 3s 3ms/step\n",
      "Iteration 3/5 - RMSE: 0.7675155053617015, MAE: 0.5942185907065868, MAPE: 0.14072376079456755\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.9823740387195546, MAE: 0.7915106664597987, MAPE: 0.17829131477656768\n",
      "7/7 [==============================] - 3s 3ms/step\n",
      "Iteration 5/5 - RMSE: 0.7228900307468604, MAE: 0.5578191519635064, MAPE: 0.13597212023988894\n",
      "Average RMSE: 0.803103510641219\n",
      "Average MAE: 0.6238406215097223\n",
      "Average MAPE: 0.14703496386680684\n"
     ]
    }
   ],
   "source": [
    "# Bi-GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#\n",
    "file_path = 'data/basic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Handle missing values (e.g., replace with 0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Extract player_id that exists in 2017, 2018, and 2019\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data for 2017 and 2018 only\n",
    "final = common_data[common_data['year'].isin([2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data format\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define Bidirectional GRU model\n",
    "    model_BiGRU = Sequential()\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n",
    "    model_BiGRU.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Flatten())\n",
    "    model_BiGRU.add(Dense(512, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dropout(rate=0.5))\n",
    "    model_BiGRU.add(Dense(64, activation=\"relu\"))\n",
    "    model_BiGRU.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_BiGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter data for 2019\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale data for 2019\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # Convert to time series data format (using 2017, 2018 data to predict 2019)\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict data for 2019\n",
    "    y_pred_scaled = model_BiGRU.predict(X_2019_seq)\n",
    "\n",
    "    # Restore scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2019 p_era values for comparison\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 1/5 - RMSE: 0.6538140464902227, MAE: 0.4872387775353023, MAPE: 0.12299396656449361\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 2/5 - RMSE: 0.6495495532154486, MAE: 0.49590648123196196, MAPE: 0.12478261531540744\n",
      "7/7 [==============================] - 6s 5ms/step\n",
      "Iteration 3/5 - RMSE: 0.6460117172183051, MAE: 0.4857761431378978, MAPE: 0.1259019625542376\n",
      "7/7 [==============================] - 3s 4ms/step\n",
      "Iteration 4/5 - RMSE: 0.6328752162196566, MAE: 0.4708396213395255, MAPE: 0.12435297581530258\n",
      "7/7 [==============================] - 4s 4ms/step\n",
      "Iteration 5/5 - RMSE: 0.6654452293615494, MAE: 0.49058008185454777, MAPE: 0.12357052282195476\n",
      "Average RMSE: 0.6495391525010364\n",
      "Average MAE: 0.486068221019847\n",
      "Average MAPE: 0.1243204086142792\n"
     ]
    }
   ],
   "source": [
    "# biattention GRU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, Bidirectional, Input, Layer, dot, concatenate, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'data/basic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(columns=['last_name, first_name'])\n",
    "\n",
    "# Handle missing values (e.g., replace with 0)\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Extract player_id that exists in 2017, 2018, and 2019\n",
    "\n",
    "data_2017 = data[data['year'] == 2017]\n",
    "data_2018 = data[data['year'] == 2018]\n",
    "data_2019 = data[data['year'] == 2019]\n",
    "\n",
    "player_ids_2017 = set(data_2017['player_id'].unique())\n",
    "player_ids_2018 = set(data_2018['player_id'].unique())\n",
    "player_ids_2019 = set(data_2019['player_id'].unique())\n",
    "\n",
    "common_player_ids = player_ids_2017 & player_ids_2018 & player_ids_2019\n",
    "\n",
    "# Extract data corresponding to common player_id\n",
    "common_data = data[data['player_id'].isin(common_player_ids)]\n",
    "\n",
    "# Extract data corresponding to 2017, 2018\n",
    "final = common_data[common_data['year'].isin([2017, 2018])]\n",
    "final = final.sort_values(by=['player_id', 'year'])\n",
    "\n",
    "# Select necessary columns (excluding year)\n",
    "features = [col for col in final.columns if col not in ['player_id', 'year', 'p_era']]\n",
    "target = 'p_era'\n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = final[features].values\n",
    "y = final[target].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to time series data format\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)):\n",
    "        seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "        seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "        seq_y = y[i]\n",
    "        X_seq.append(seq_x)\n",
    "        y_seq.append(seq_y)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 2  # Set sequence length\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
    "\n",
    "# Set training data same as entire data\n",
    "X_train, y_train = X_seq, y_seq\n",
    "\n",
    "# Define custom Attention Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "iterations = 5\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define BiAttention GRU model\n",
    "    input_seq = Input(shape=(seq_length, X_train.shape[2]))\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(input_seq)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    # Apply Attention Layer\n",
    "    attn_out = Attention()(x)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation=\"relu\")(attn_out)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model_BiAttGRU = Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model_BiAttGRU.compile(loss=\"mse\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_BiAttGRU.fit(X_train, y_train, epochs=500, batch_size=64, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Filter 2023 data\n",
    "    data_19 = common_data[common_data['year'] == 2019]\n",
    "\n",
    "    # Scale 2023 data\n",
    "    X_2019_scaled = scaler_X.transform(data_19[features].values)\n",
    "    \n",
    "    def create_sequences_for_prediction(X, seq_length):\n",
    "        X_seq = []\n",
    "        for i in range(len(X)):\n",
    "            seq_x = X[max(0, i - seq_length + 1):i + 1]\n",
    "            seq_x = np.pad(seq_x, ((seq_length - len(seq_x), 0), (0, 0)), 'constant')\n",
    "            X_seq.append(seq_x)\n",
    "        return np.array(X_seq)\n",
    "\n",
    "    # Convert to time series data format\n",
    "    X_2019_seq = create_sequences_for_prediction(X_2019_scaled, seq_length)\n",
    "\n",
    "    # Predict 2019 data\n",
    "    y_pred_scaled = model_BiAttGRU.predict(X_2019_seq)\n",
    "\n",
    "    # Restore scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Restore actual 2019 p_era values for comparison\n",
    "    y_test_actual = data_19[target].values\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual[:len(y_pred)], y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_actual[:len(y_pred)], y_pred)\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    print(f'Iteration {i+1}/{iterations} - RMSE: {rmse}, MAE: {mae}, MAPE: {mape}')\n",
    "\n",
    "# Calculate average RMSE, MAE, MAPE\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mape = np.mean(mape_list)\n",
    "\n",
    "print(f'Average RMSE: {avg_rmse}')\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MAPE: {avg_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
